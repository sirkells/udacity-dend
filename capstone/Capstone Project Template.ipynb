{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycountry in /opt/conda/lib/python3.6/site-packages (19.8.18)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "from datetime import datetime, timedelta\n",
    "import os, glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from utility import get_files, get_name, map_country, is_empty, has_null, data_quality_check, data_type, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "root = [\"../../\", \"/data/18-83510-I94-Data-2016/\"]\n",
    "\n",
    "# Source Data Paths\n",
    "source_immigration_data = \"../../data/18-83510-I94-Data-2016/\"\n",
    "source_cities_dem = 'source_data/us-cities-demographics.csv'\n",
    "source_country = 'source_data/countries.csv'\n",
    "source_country_mapping = 'source_data/country-mapping.txt'\n",
    "source_airport = 'source_data/airport.txt'\n",
    "\n",
    "# Staging Data Paths\n",
    "staging_immigration_path = 'staging_files/immigration_data/'\n",
    "staging_airport_path = 'staging_files/airport_data/'\n",
    "staging_cities_dem_path = 'staging_files/us-cities-demograpy/'\n",
    "staging_country_path = 'staging_files/country_data/'\n",
    "\n",
    "# Output Parquet Data Paths\n",
    "airport_output_path = 'tables/airport/'\n",
    "country_output_path = 'tables/country/'\n",
    "passenger_output_path = 'tables/passenger/'\n",
    "time_output_path = 'tables/time/'\n",
    "entry_output_path = 'tables/entry/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    \"\"\"\n",
    "     Create or retrieve a Spark Session\n",
    "    \"\"\"\n",
    "\n",
    "    spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                        \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# User defined spark functions\n",
    "countries = map_country(source_country_mapping)\n",
    "@udf(TimestampType())\n",
    "def get_timestamp(arrdate):\n",
    "    return datetime(1960, 1, 1) + timedelta(days=int(arrdate))\n",
    "@udf(StringType())\n",
    "def get_country(code):\n",
    "    code = int(code)\n",
    "    #print(code)\n",
    "    return countries[code] if code in countries else \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Project Scope , Data Descriptions and Sources\n",
    "\n",
    "#### Scope \n",
    "\n",
    "- The aim of this project is to build an ETL pipeline that extracts data from the dataset on immigration to the United States, and supplementry airport and country codes datasets. \n",
    "    \n",
    "- Process, transform the extracted datasets and load the data back into a file storage as a set of tables in parquet format.\n",
    "\n",
    "- The end solution would be a star schema model with one fact table and 4 dimension tables suitable for analytics use cases\n",
    "\n",
    "- From these tables, further analytics can be carried out\n",
    "\n",
    "- Tools used: Python, Pandas, Pyspark, Pycountry\n",
    "\n",
    "#### Description of Data and Sources\n",
    "\n",
    "* ../..data/18-83510-I94-Data-2016/: US I94 immigration data from 2016 (Jan-Dec).\n",
    "    * Data Source: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "    * I94_SAS_Labels_Descriptions.SAS file contains data dictionary for the dataset.\n",
    "    * The dataset is divided into files with each file representing the events for each month (jan-dec) in 2016\n",
    "    * Each file contains about 3M rows\n",
    "    * Data has 28 columns containing information about airport, airline, countries, cities, time, flight number etc.\n",
    "    \n",
    "* source_data/airport.txt: Airport code, city name and city code extracted from the I94_SAS_Labels_Descriptions.SAS file\n",
    "    * Data Source: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "    * Description: Data contains information about different airports code, name and city codes\n",
    "    * Data has 660 rows and 3 columns rep airport code, airport name and city code.\n",
    "\n",
    "* source_data/countries.csv: ISO-3166 Country and Dependent Territories Lists with UN Regional Codes\n",
    "    * Source: https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes\n",
    "    * Description: Contains data for alpha and numeric country codes, and the UN Statistics data for countries regional, and sub-regional codes\n",
    "    * Data has 249 rows and 11 columns rep several types of country and regional codes  \n",
    "\n",
    "* source_data/country-mapping.txt: I94 Country Code extracted from the I94_SAS_Labels_Descriptions.SAS file\n",
    "    * Source: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "    * Description: Data contains country names and code\n",
    "    * Data has 236 rows and 2 columns rep country code and name\n",
    "\n",
    "\n",
    "* NOTE: two country data sources were used and combined. \n",
    "    * The I94 data only contained country code and name and in combination with the ISO-3166 Country data, was used to extract alpha_2, alpha_3 and regional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Data Exploration\n",
    "\n",
    "#### Data Quality Issues:\n",
    "    \n",
    "- US I94 Immigration data: \n",
    "    * Missing and null row values\n",
    "    * Different data types value in some columns\n",
    "    \n",
    "- US I94 Airport data: \n",
    "    * Inconsistent data type in columns\n",
    "    * Data values contains several punctuation marks and white spaces\n",
    "    \n",
    "- US I94 Country code data: \n",
    "    * Data values contains several punctuation marks and white spaces\n",
    "    * Invalid and redundant values\n",
    "\n",
    "- ISO-3166 Country and Dependent Territories Data:\n",
    "    * Missing and null row values\n",
    "    * Country codes not consistent with US I94 Data Values\n",
    "\n",
    "#### Steps in Cleaning Data\n",
    "\n",
    "- US I94 Immigration data: \n",
    "    * Replace all missing and null row values with 'NA' (string) and 0.0 (double)\n",
    "    * Convert data type to suitable types based on row values\n",
    "    \n",
    "- US I94 Airport data: \n",
    "    * Convert data type to suitable types based on row values\n",
    "    * Remove punctuation marks and white spaces\n",
    "    \n",
    "- US I94 Country code data: \n",
    "    * Convert data type to suitable types based on row values\n",
    "    * Removed invalid and redundant values\n",
    "\n",
    "- ISO-3166 Country and Dependent Territories Data:\n",
    "    * Replace all missing and null row values with 'others'(string) and 0.0(double)\n",
    "    * Extract data values consistent with US I94 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cleaning US I94 Immigration data\n",
    "\n",
    "# create the function\n",
    "\n",
    "def clean_staging_immigration_data(spark, file_path, target):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to read, clean and copy the immigraion files from the source file_path,\n",
    "    loads it into the staging directory as parquet files\n",
    "\n",
    "    Arguments:\n",
    "        spark: spark session\n",
    "        file_path: input immigration data file path.\n",
    "        target: destination directory\n",
    "\n",
    "    Returns:\n",
    "        data: spark df\n",
    "    \"\"\"\n",
    "\n",
    "    default_missing_values = {\n",
    "        'i94mode': 0.0,\n",
    "        'i94addr': 'NA',\n",
    "        'depdate': 0.0,\n",
    "        'i94bir': 'NA',\n",
    "        'i94visa': 0.0,\n",
    "        'count': 0.0,\n",
    "        'dtadfile': 'NA',\n",
    "        'visapost': 'NA',\n",
    "        'occup': 'NA',\n",
    "        'entdepa': 'NA',\n",
    "        'entdepd': 'NA',\n",
    "        'entdepu': 'NA',\n",
    "        'matflag': 'NA',\n",
    "        'biryear': 0.0,\n",
    "        'dtaddto': 'NA',\n",
    "        'gender': 'NA',\n",
    "        'insnum': 'NA',\n",
    "        'airline': 'NA',\n",
    "        'admnum': 0.0,\n",
    "        'fltno': 'NA',\n",
    "        'visatype': 'NA'\n",
    "    }\n",
    "\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark').load(file_path)\n",
    "    df = df.na.fill(default_missing_values)\n",
    "    df.write.mode(\"overwrite\").parquet(target)\n",
    "    df = spark.read.parquet(target)\n",
    "    print(f\"Cleaning and Staging Completed\")\n",
    "    df.show(5)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and Staging Completed\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD|   NA|      G|      O|     NA|      M| 1976.0|10292016|     F|    NA|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD|   NA|      G|      O|     NA|      M| 1984.0|10292016|     F|    NA|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD|   NA|      G|      O|     NA|      M| 1987.0|10292016|     M|    NA|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD|   NA|      G|      O|     NA|      M| 1987.0|10292016|     F|    NA|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD|   NA|      G|      O|     NA|      M| 1988.0|10292016|     M|    NA|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cicid: double, i94yr: double, i94mon: double, i94cit: double, i94res: double, i94port: string, arrdate: double, i94mode: double, i94addr: string, depdate: double, i94bir: double, i94visa: double, count: double, dtadfile: string, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, biryear: double, dtaddto: string, gender: string, insnum: string, airline: string, admnum: double, fltno: string, visatype: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean and stage\n",
    "directory = get_files(root)[0]\n",
    "clean_staging_immigration_data(spark, directory, staging_immigration_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cleaning US I94 Airport data \n",
    "def clean_staging_airport_data(spark, file_path, target):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to read, clean and copy the airport data file_path and\n",
    "    loads it into the staging directory\n",
    "\n",
    "    Arguments:\n",
    "        spark: spark session\n",
    "        file_path: input data file path.\n",
    "        target: destination directory\n",
    "\n",
    "    Returns:\n",
    "        data: spark df\n",
    "    \"\"\"\n",
    "    print(\"Staging Started\")\n",
    "    name_arr, code_arr, location_arr = list(), list(), list()\n",
    "    \n",
    "    # read airport data\n",
    "    with open(file_path) as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            # remove punctuations and white spaces\n",
    "            code, name = line.split('=')\n",
    "            code = code.strip().strip().replace(\"'\", \"\")\n",
    "            name = \" \".join(name.strip().replace(\"'\", \"\").split()).split(',')\n",
    "            code_arr.append(code)\n",
    "            \n",
    "            if len(name) == 2:\n",
    "                name1 = name[0]\n",
    "                loc = name[1].strip() \n",
    "            else:\n",
    "                loc = code\n",
    "                name1 = code\n",
    "                \n",
    "            name_arr.append(name1)\n",
    "            location_arr.append(loc)\n",
    "            \n",
    "    data = list(zip(code_arr, name_arr, location_arr))\n",
    "\n",
    "    df_airport = pd.DataFrame(data, columns=['airport_id', 'airport_name', 'airport_location'])\n",
    "    df_airport['airport_id'] = df_airport['airport_id'].astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "    df_airport['airport_name'] = df_airport['airport_name'].astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "    df_airport['airport_location'] = df_airport['airport_location'].astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "    #df_airport.head(10)\n",
    "    \n",
    "    # save csv\n",
    "    #df_airport.to_csv(target + 'airport.csv', index=False)\n",
    "\n",
    "    output = target + 'parquet/'\n",
    "    print(\"Spark Processing\")\n",
    "    schema = StructType([\n",
    "        StructField(\"airport_id\", StringType(), False),\n",
    "        StructField(\"airport_name\", StringType(), False),\n",
    "        StructField(\"airport_location\", StringType(), False)\n",
    "    ])\n",
    "    df_spark = spark.createDataFrame(df_airport, schema=schema)\n",
    "\n",
    "    df_spark.write.mode(\"overwrite\").parquet(output)\n",
    "    df_spark = spark.read.parquet(output)\n",
    "    df_spark.show(5)\n",
    "    print(f\"Staging Completed \")\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging Started\n",
      "Spark Processing\n",
      "+----------+--------------------+----------------+\n",
      "|airport_id|        airport_name|airport_location|\n",
      "+----------+--------------------+----------------+\n",
      "|       ALC|               ALCAN|              AK|\n",
      "|       ANC|           ANCHORAGE|              AK|\n",
      "|       BAR|BAKER AAF - BAKER...|              AK|\n",
      "|       DAC|       DALTONS CACHE|              AK|\n",
      "|       PIZ|DEW STATION PT LA...|              AK|\n",
      "+----------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Staging Completed \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[airport_id: string, airport_name: string, airport_location: string]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_staging_airport_data(spark, source_airport, staging_airport_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean, merge and stage ISO-3166 and US I94 country data\n",
    "def clean_staging_country_data(spark, path1, path2, target):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to read, clean and copy the country data file and\n",
    "    loads it into the staging directory\n",
    "\n",
    "    Arguments:\n",
    "        spark: spark session\n",
    "        path1: file path to countries.csv file downloaded from source https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes\n",
    "        path2: file path to country.mapping.txt file gotten from I94_SAS_Labels_Descriptions\n",
    "        target: destination directory\n",
    "\n",
    "    Returns:\n",
    "        data: country spark df\n",
    "    \"\"\"\n",
    "    print(\"Staging Started\")\n",
    "    \n",
    "    # get ISO-3166 data\n",
    "    df1 = pd.read_csv(path1)\n",
    "    \n",
    "    # get US I94 country data\n",
    "    countries = map_country(path2)\n",
    "    \n",
    "    df2 = pd.DataFrame(list(countries.items()), columns=['code', 'name'])\n",
    "    df2 = df2[df2.name != 'Kosovo']\n",
    "    df2 = df2[df2.name != 'Zaire']\n",
    "\n",
    "    name_1, alpha_2, alpha_3, official_name, official_code = list(), list(), list(), list(), list()\n",
    "    \n",
    "    # Extract alpha_2, alpha_3, official_name, official_code from ISO-3166 data\n",
    "    for i, row in df2.iterrows():\n",
    "        b, c, d, e, = get_name(row['name'])\n",
    "        alpha_2.append(b)\n",
    "        alpha_3.append(c)\n",
    "        official_name.append(d)\n",
    "        official_code.append(e)\n",
    "    \n",
    "    # Add extracted column and row data to US I94 data\n",
    "    df2['alpha_2'] = alpha_2\n",
    "    df2['alpha_3'] = alpha_3\n",
    "    df2['official_name'] = official_name\n",
    "    df2['official_code'] = official_code\n",
    "\n",
    "    df2['region'] = df2['alpha_3'].apply(lambda x: df1[df1['alpha-3'] == x].region.values)\n",
    "    df2['region'] = df2['region'].astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "    \n",
    "    # save csv\n",
    "    #df2.to_csv(target + 'csv/country_processed.csv', index=False)\n",
    "\n",
    "    output = target + 'parquet/'\n",
    "\n",
    "    print(\"Spark Processing\")\n",
    "    \n",
    "    # set data types\n",
    "    schema = StructType([\n",
    "        StructField(\"code\", IntegerType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"alpha_2\", StringType(), True),\n",
    "        StructField(\"alpha_3\", StringType(), True),\n",
    "        StructField(\"official_name\", StringType(), True),\n",
    "        StructField(\"official_code\", StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "\n",
    "    ])\n",
    "    df_spark = spark.createDataFrame(df2, schema=schema)\n",
    "    df_spark.write.mode(\"overwrite\").parquet(output)\n",
    "    df_spark = spark.read.parquet(output)\n",
    "    df_spark.show(5)\n",
    "    print(f\"Staging Completed \")\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging Started\n",
      "Spark Processing\n",
      "+----+-----------+-------+-------+--------------------+-------------+------+\n",
      "|code|       name|alpha_2|alpha_3|       official_name|official_code|region|\n",
      "+----+-----------+-------+-------+--------------------+-------------+------+\n",
      "| 236|Afghanistan|     AF|    AFG|Islamic Republic ...|          004|  Asia|\n",
      "| 101|    Albania|     AL|    ALB| Republic of Albania|          008|Europe|\n",
      "| 316|    Algeria|     DZ|    DZA|People's Democrat...|          012|Africa|\n",
      "| 102|    Andorra|     AD|    AND|Principality of A...|          020|Europe|\n",
      "| 324|     Angola|     AO|    AGO|  Republic of Angola|          024|Africa|\n",
      "+----+-----------+-------+-------+--------------------+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Staging Completed \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[code: int, name: string, alpha_2: string, alpha_3: string, official_name: string, official_code: string, region: string]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_staging_country_data(spark, source_country, source_country_mapping, staging_country_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "I chose a Star Schema model for the US I94 Immigration Data because:\n",
    "\n",
    "- Easy to design\n",
    "\n",
    "- The schema design is suitable for the earlier defined use case (Analytics) purposes\n",
    "\n",
    "- The star chema model contains the following tables\n",
    "\n",
    "    - Facts Table:\n",
    "        - entry table\n",
    "        \n",
    "    - Dimension Tables:\n",
    "        - airport table: \n",
    "        - country table: \n",
    "        - passenger table\n",
    "        - time table\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "- ETL pipeline script **etl.py** reads and extracts data from Source Data Paths\n",
    "\n",
    "- Extracted data is cleaned and loaded in spark parquet files into staging paths\n",
    "\n",
    "- Spark processes and transforms data into one fact table (entry partitioned by year and month) and four Dimension Tables; country, time(partitioned by year and month), passenger and airport tables with each having the right columns and data types.\n",
    "\n",
    "- Loads and writes processed tables in parquet form into output directories where the analytics team can further find insights.\n",
    "\n",
    "- Data quality checks are run at the end for each table the validate tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- airport_name: string (nullable = true)\n",
      " |-- airport_region: string (nullable = true)\n",
      "\n",
      "+----------+------------------+--------------+\n",
      "|airport_id|      airport_name|airport_region|\n",
      "+----------+------------------+--------------+\n",
      "|       EPT|EASTPORT MUNICIPAL|            ME|\n",
      "|       SPA|        ST PAMPILE|            ME|\n",
      "|       GTF|Collapsed into INT|            MN|\n",
      "+----------+------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Airport Data Model\n",
    "df = spark.read.parquet(staging_airport_path+'parquet/')\n",
    "df.createOrReplaceTempView(\"airport\")\n",
    "query = \"\"\"\n",
    "\n",
    "    SELECT DISTINCT airport_id, \n",
    "                    airport_name,  \n",
    "                    airport_location as airport_region\n",
    "    FROM airport\n",
    "    \"\"\"\n",
    "airport_df = spark.sql(query)\n",
    "airport_df.printSchema()\n",
    "airport_df.write.mode(\"overwrite\").parquet(airport_output_path)\n",
    "airport_df = spark.read.parquet(airport_output_path)\n",
    "airport_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_id: integer (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- alpha_2: string (nullable = true)\n",
      " |-- alpha_3: string (nullable = true)\n",
      " |-- official_name: string (nullable = true)\n",
      " |-- iso_code: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+----------+--------------------+-------+-------+--------------------+--------+--------+\n",
      "|country_id|        country_name|alpha_2|alpha_3|       official_name|iso_code|  region|\n",
      "+----------+--------------------+-------+-------+--------------------+--------+--------+\n",
      "|       717|Bonaire, sint eus...|     BQ|    BES|Bonaire, Sint Eus...|     535|Americas|\n",
      "|       330|Sao tome and prin...|     ST|    STP|Democratic Republ...|     678|  Africa|\n",
      "|       296|United arab emirates|   null|   null|                null|    null|        |\n",
      "+----------+--------------------+-------+-------+--------------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Country Data Model\n",
    "df = spark.read.parquet(staging_country_path+'parquet/')\n",
    "df.createOrReplaceTempView(\"country\")\n",
    "query = \"\"\"\n",
    "\n",
    "SELECT DISTINCT code as country_id, \n",
    "                name as country_name,  \n",
    "                alpha_2 , alpha_3, official_name,\n",
    "                official_code as iso_code, \n",
    "                region\n",
    "FROM country\n",
    "ORDER BY country_name\n",
    "\"\"\"\n",
    "country_df = spark.sql(query)\n",
    "country_df.printSchema()\n",
    "country_df.write.mode(\"overwrite\").parquet(country_output_path)\n",
    "country_df = spark.read.parquet(country_output_path)\n",
    "country_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_timestamp(arrdate)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"get_country\", get_country)\n",
    "spark.udf.register(\"get_timestamp\", get_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_id: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|            time_id|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2016-04-10 00:00:00|   0| 10|  14|    4|2016|      1|\n",
      "|2016-04-16 00:00:00|   0| 16|  15|    4|2016|      7|\n",
      "|2016-04-30 00:00:00|   0| 30|  17|    4|2016|      7|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time Data Model\n",
    "\n",
    "df = spark.read.parquet(staging_immigration_path).withColumn(\"time_id\", get_timestamp(\"arrdate\"))\n",
    "df.createOrReplaceTempView(\"time\")\n",
    "query = \"\"\"\n",
    "\n",
    "SELECT DISTINCT  time_id, hour(time_id) as hour, day(time_id) as day,\n",
    "                 weekofyear(time_id) as week, month(time_id) as month, year(time_id) as year,\n",
    "                 dayofweek(time_id) weekday\n",
    "\n",
    "FROM time\n",
    "ORDER BY time_id\n",
    "\"\"\"\n",
    "time_df = spark.sql(query)\n",
    "time_df.printSchema()\n",
    "time_df.write.mode(\"overwrite\").parquet(time_output_path)\n",
    "time_df = spark.read.parquet(time_output_path)\n",
    "time_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_id: double (nullable = true)\n",
      " |-- flight_no: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- time_of_arrival: timestamp (nullable = true)\n",
      " |-- departure_country: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- purpose_of_travel: double (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "+---------------+---------+-------+-------+-------------------+-----------------+------+-----------------+----+\n",
      "|   passenger_id|flight_no|airline|airport|    time_of_arrival|departure_country|gender|purpose_of_travel| age|\n",
      "+---------------+---------+-------+-------+-------------------+-----------------+------+-----------------+----+\n",
      "|5.9564187033E10|    00657|     FI|    SPM|2016-04-30 00:00:00|          Finland|     F|              1.0|55.0|\n",
      "|5.9564188933E10|    02083|     AA|    CLT|2016-04-30 00:00:00|      Netherlands|     M|              2.0| 5.0|\n",
      "|5.9564189833E10|    00615|     FI|    NYC|2016-04-30 00:00:00|          Iceland|     M|              2.0|55.0|\n",
      "+---------------+---------+-------+-------+-------------------+-----------------+------+-----------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passenger Data Model\n",
    "\n",
    "#df = spark.read.parquet(staging_immigration_path).withColumn(\"time_id\", get_timestamp(\"arrdate\"))\n",
    "df.createOrReplaceTempView(\"passenger\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "SELECT DISTINCT admnum as passenger_id, string(fltno) as flight_no, airline, i94port as airport, time_id as time_of_arrival,\n",
    "                get_country(i94cit) as departure_country, gender, i94visa as purpose_of_travel, i94bir as age\n",
    "FROM passenger\n",
    "WHERE get_country(i94cit) != 'Others'\n",
    "ORDER BY passenger_id\n",
    "\"\"\"\n",
    "passenger_df = spark.sql(query)\n",
    "passenger_df.printSchema()\n",
    "passenger_df.write.mode(\"overwrite\").parquet(passenger_output_path)\n",
    "passenger_df = spark.read.parquet(passenger_output_path)\n",
    "passenger_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- entry_id: integer (nullable = true)\n",
      " |-- airline_id: string (nullable = true)\n",
      " |-- flight_id: string (nullable = true)\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- passenger_id: double (nullable = true)\n",
      " |-- country_id: string (nullable = true)\n",
      " |-- time_id: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+--------+----------+---------+----------+---------------+----------+-------------------+----+---+----+-----+----+-------+\n",
      "|entry_id|airline_id|flight_id|airport_id|   passenger_id|country_id|            time_id|hour|day|week|month|year|weekday|\n",
      "+--------+----------+---------+----------+---------------+----------+-------------------+----+---+----+-----+----+-------+\n",
      "| 5417904|        OS|    00089|       NEW|5.9451939333E10|   Austria|2016-04-29 00:00:00|   0| 29|  17|    4|2016|      6|\n",
      "| 5418156|        BA|    00269|       LOS|5.9503382733E10|   Austria|2016-04-29 00:00:00|   0| 29|  17|    4|2016|      6|\n",
      "| 5418539|        LX|    00066|       MIA|5.9473817433E10|   Austria|2016-04-29 00:00:00|   0| 29|  17|    4|2016|      6|\n",
      "+--------+----------+---------+----------+---------------+----------+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entry Data Model\n",
    "#df = spark.read.parquet(staging_immigration_path).withColumn(\"time_id\", get_timestamp(\"arrdate\"))\n",
    "df.createOrReplaceTempView(\"entry\")\n",
    "entry_df = spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct int(cicid) as entry_id,\n",
    "                    airline as airline_id,\n",
    "                    string(fltno) as flight_id,\n",
    "                    i94port as airport_id, \n",
    "                    admnum as passenger_id, \n",
    "                    get_country(i94cit) as country_id,\n",
    "                    time_id, \n",
    "                    hour(time_id) as hour, \n",
    "                    day(time_id) as day,\n",
    "                    weekofyear(time_id) as week,\n",
    "                    month(time_id) as month, \n",
    "                    year(time_id) as year,\n",
    "                    dayofweek(time_id) as weekday\n",
    "    from entry\n",
    "    ORDER BY time_id ASC\n",
    "    \"\"\"\n",
    ")\n",
    "entry_df.printSchema()\n",
    "entry_df.write.mode(\"overwrite\").parquet(entry_output_path)\n",
    "entry_df = spark.read.parquet(entry_output_path)\n",
    "entry_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data quality checks:\n",
    "\n",
    " * Check that fact and dimension tables are not empty\n",
    " * Check that fact and dimension tables have no null values\n",
    " * Check that primary and secondary keys have the correct data type\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_quality_check(df, metric: dict, table):\n",
    "    \"\"\"\n",
    "    Description: This function checks if spark df is empty, null and has wrong data types\n",
    "\n",
    "    Arguments:\n",
    "        df: spark data frame\n",
    "        metric: dict containing:\n",
    "                query: sql query to check if df is null\n",
    "                dtype1, dtype2: expected data types in columns (name1, name2)\n",
    "                name1, name2: columns names\n",
    "\n",
    "    Returns:\n",
    "         status: dict => msg: Status, code: 1 or 0\n",
    "    \"\"\"\n",
    "    table_is_empty = is_empty(df)\n",
    "    print(f\"Passed: {table} Table is Not Empty\")\n",
    "    table_has_null = has_null(df, metric[\"query\"])\n",
    "    print(f\"Passed: {table} Table has No Null Values\")\n",
    "    col1_has_wrong_data_type = data_type(df, metric['dtype1'], metric[\"name1\"])\n",
    "    print(f\"Passed: Data type for {metric['name1']} column is correct\")\n",
    "    col2_has_wrong_data_type = data_type(df, metric['dtype2'], metric[\"name2\"])\n",
    "    print(f\"Passed: Data type for {metric['name2']} column is correct\")\n",
    "    results = {table_is_empty, table_has_null, col1_has_wrong_data_type, col2_has_wrong_data_type}\n",
    "\n",
    "    #status = \"Failed\" if 1 in results else \"Passed\"\n",
    "    if 1 in results:\n",
    "        status = {\n",
    "            \"msg\": \"Failed\",\n",
    "            \"Code\": 1\n",
    "        }\n",
    "    else:\n",
    "        status = {\n",
    "            \"msg\": \"Passed\",\n",
    "            \"Code\": 0\n",
    "        }\n",
    "\n",
    "    if table_is_empty:\n",
    "        print(\"Err: Table is Empty\")\n",
    "    elif table_has_null:\n",
    "        print(\"Err: Table has Null\")\n",
    "    elif col1_has_wrong_data_type:\n",
    "        print(f\"Err: {metrics['name1']} should be {metrics['dtype1']} type\")\n",
    "    elif col2_has_wrong_data_type:\n",
    "        print(f\"Err: {metrics['name2']} should be {metrics['dtype2']} type\")\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: Airport Table is Not Empty\n",
      "Passed: Airport Table has No Null Values\n",
      "Passed: Data type for airport_id column is correct\n",
      "Passed: Data type for airport_name column is correct\n",
      "Airport table check Passed\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "# Airport Table check\n",
    "try:\n",
    "    airport_table_check = data_quality_check(airport_df,metrics([\"airport_id\", \"airport_name\"], [\"string\", \"string\"]), table=\"Airport\")\n",
    "    print(\"Airport table check Passed\")\n",
    "except Exception as e:\n",
    "    print(\"Airport table check failed\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: Counry Table is Not Empty\n",
      "Passed: Counry Table has No Null Values\n",
      "Passed: Data type for country_id column is correct\n",
      "Passed: Data type for country_name column is correct\n",
      "Country table check Passed\n"
     ]
    }
   ],
   "source": [
    "# Country Table check\n",
    "try:\n",
    "    country_table_check = data_quality_check(country_df, metrics([\"country_id\", \"country_name\"], [\"int\", \"string\"]),table=\"Counry\")\n",
    "    print(\"Country table check Passed\")\n",
    "except Exception as e:\n",
    "    print(\"Country table check failed\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: Passenger Table is Not Empty\n",
      "Passed: Passenger Table has No Null Values\n",
      "Passed: Data type for passenger_id column is correct\n",
      "Passed: Data type for flight_no column is correct\n",
      "Passenger table check Passed\n"
     ]
    }
   ],
   "source": [
    "# Passenger Table check\n",
    "try:\n",
    "    passenger_table_check = data_quality_check(passenger_df,metrics([\"passenger_id\", \"flight_no\"], [\"double\", \"string\"]), table=\"Passenger\")\n",
    "    print(\"Passenger table check Passed\")\n",
    "except Exception as e:\n",
    "    print(\"Passenger table check failed\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: Time Table is Not Empty\n",
      "Passed: Time Table has No Null Values\n",
      "Passed: Data type for time_id column is correct\n",
      "Passed: Data type for hour column is correct\n",
      "Time table check Passed\n"
     ]
    }
   ],
   "source": [
    "# Time Table check\n",
    "try:\n",
    "    time_table_check = data_quality_check(time_df, metrics([\"time_id\", \"hour\"], [\"timestamp\", \"int\"]), table=\"Time\")\n",
    "    print(\"Time table check Passed\")\n",
    "except Exception as e:\n",
    "    print(\"Time table check failed\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: Entry Table is Not Empty\n",
      "Passed: Entry Table has No Null Values\n",
      "Passed: Data type for time_id column is correct\n",
      "Passed: Data type for entry_id column is correct\n",
      "Entry table check Passed\n"
     ]
    }
   ],
   "source": [
    "# Entry Table check\n",
    "try:\n",
    "    entry_table_check = data_quality_check(entry_df, metrics([\"time_id\", \"entry_id\"], [\"timestamp\", \"int\"]), table=\"Entry\")\n",
    "    print(\"Entry table check Passed\")\n",
    "except Exception as e:\n",
    "    print(\"Entry table check failed\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality check on all tables was successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_checks = {airport_table_check['Code'], country_table_check['Code'], passenger_table_check['Code'],\n",
    "                  time_table_check['Code'], entry_table_check['Code']}\n",
    "\n",
    "if 1 in all_checks:\n",
    "    print(\"Data Quality Failed\")\n",
    "else:\n",
    "    print(\"Data Quality check on all tables was successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data dictionary can be found in the **data_dictionary.txt** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### Step 5: Rationale for tools chosen:\n",
    "- Python: flexible and easy to use\n",
    "- Pandas: i am comfortable with python and pandas is a python library optimized for data analysis and manipulation\n",
    "- Spark: Has a python api called pyspark. Also has a dataframe like pandas and allows sql queries. Suitable for data with over 1 million rows\n",
    "- Local Storage: Ideally AWS S3 and Redshift would be prefered choices for files and data storage. But i used local storage because of the limited amount of data. \n",
    "\n",
    "\n",
    "##### How often should ETL scripts be run?\n",
    "- Based on the use cases and assuming the US I94 Immigration data is made available on monthly, i would propose the ETL script should be run monthly.\n",
    "   \n",
    "\n",
    "##### Possible scenarions\n",
    "- Data is 100x: \n",
    "    - Source Data files should be stoted in AWS S3 or other cloud storage\n",
    "    - Multi node Spark cluster with data stored in a distributed file system like HDFS should be used to enable faster and parallel data processing.\n",
    "    - Output data (Dimensions and Fact Tables) as parquet files should be stored in AWS s3 or any cheaper cloud storage system for easy access\n",
    "\n",
    "##### Data populates a dashboard and updated every day 07:00AM:\n",
    "- The ETL script should only process changes detected in input data and update the dashboard database instead of processing  all data. \n",
    "\n",
    "##### DB is accessed by 100+ people:\n",
    "- Adding more dimension tables to cater for several users use cases\n",
    "- A cloud data warehouse should be used so that data would always be available to users easily. \n",
    "\n",
    "##### Future Work/ Improvements \n",
    "- Apache Airflow could be used toautomatically monitor and schedule pipelines\n",
    "- US-cities-demographics data could be added to show the demography of differebÂ´nt cities visisted by different countries for example are flights from africa more directed to cities in the us with high african americans? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
